import re
import nltk
import numpy as np
from nltk.tokenize import word_tokenize
from pdfminer.high_level import extract_text
from sentence_transformers import SentenceTransformer
import argparse


# –ó–∞–≥—Ä—É–∑–∏–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä NLTK
nltk.download('punkt_tab')
nltk.download('stopwords')
nltk.download('averaged_perceptron_tagger')
nltk.download('wordnet')
nltk.download('omw-1.4')


def extract_text_pdfminer(pdf_path):
    """ –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç —Å –ø–æ–º–æ—â—å—é pdfminer.six """
    return extract_text(pdf_path)


def split_text_custom(text):
    """
    –î–µ–ª–∏–º —Ç–µ–∫—Å—Ç –ø–æ \n\n, –≤–Ω—É—Ç—Ä–∏ –∫–∞–∂–¥–æ–≥–æ –±–ª–æ–∫–∞ —É–±–∏—Ä–∞–µ–º \n,
    –∑–∞—Ç–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –¥–µ–ª–∏–º –ø–æ \x0c –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–ø–∏—Å–æ–∫ —á–∏—Å—Ç—ã—Ö –∞–±–∑–∞—Ü–µ–≤.
    """
    # 1) –î–µ–ª–∏–º –ø–æ \n\n
    raw_blocks = text.split("\n\n")

    final_paragraphs = []

    for block in raw_blocks:
        # 2) –£–±–∏—Ä–∞–µ–º –æ–¥–∏–Ω–∞—Ä–Ω—ã–µ –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫ (–∑–∞–º–µ–Ω—è–µ–º \n –Ω–∞ –ø—Ä–æ–±–µ–ª)
        block = block.replace("\n", " ")

        # 3) –î–µ–ª–∏–º –ø–æ \x0c –∏ –¥–æ–±–∞–≤–ª—è–µ–º –≤ –æ–±—â–∏–π —Å–ø–∏—Å–æ–∫
        sub_blocks = block.split("\x0c")
        for sb in sub_blocks:
            sb = sb.strip()
            if sb:  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
                final_paragraphs.append(sb)

    return final_paragraphs


def show_paragraph_lengths(paragraphs):
    """
    –í—ã–≤–æ–¥–∏—Ç –¥–ª–∏–Ω—É –∫–∞–∂–¥–æ–≥–æ –∞–±–∑–∞—Ü–∞ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É.
    """
    max_len = 0
    for i, para in enumerate(paragraphs, start=1):
        length = len(para)
        print(f"–ê–±–∑–∞—Ü {i}, –¥–ª–∏–Ω–∞: {length}")
        if length > max_len:
            max_len = length
    return max_len


def merge_heading_paragraphs(paragraphs):
    """
    –ò—â–µ—Ç –∞–±–∑–∞—Ü—ã, –∫–æ—Ç–æ—Ä—ã–µ –Ω–∞—á–∏–Ω–∞—é—Ç—Å—è —Å:
      - —Ü–∏—Ñ—Ä
      - –¥–∞–ª–µ–µ 0+ –≥—Ä—É–ø–ø (.—Ü–∏—Ñ—Ä—ã)
      - –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è —Ç–æ—á–∫–∞
      - –ø—Ä–æ–±–µ–ª(—ã)
    –∏ –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –∏—Ö —Å –ø–æ—Å–ª–µ–¥—É—é—â–∏–º –∞–±–∑–∞—Ü–µ–º –≤ –æ–¥–∏–Ω.
    """
    # –ü—Ä–∏–º–µ—Ä: "1. Introduction" –∏–ª–∏ "1.2 Something" –∏–ª–∏ "3.4.5. Conclusion"
    # –û–±—ä—è—Å–Ω–µ–Ω–∏–µ:
    #  ^                 # –Ω–∞—á–∞–ª–æ —Å—Ç—Ä–æ–∫–∏
    #  [0-9]+           # –æ–¥–Ω–∞ –∏–ª–∏ –±–æ–ª–µ–µ —Ü–∏—Ñ—Ä
    #  (?:\.[0-9]+)*    # 0 –∏–ª–∏ –±–æ–ª–µ–µ –≥—Ä—É–ø–ø –∏–∑: —Ç–æ—á–∫–∞ + —Ü–∏—Ñ—Ä—ã
    #  (?:\.)?          # –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è —Ç–æ—á–∫–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 1.2.3.)
    #  \s+              # –ø—Ä–æ–±–µ–ª(—ã)
    heading_pattern = re.compile(r'^[0-9]+(?:\.[0-9]+)*(?:\.)?\s+')

    merged_paragraphs = []
    i = 0
    n = len(paragraphs)

    while i < n:
        current_para = paragraphs[i].strip()

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –µ—Å—Ç—å —Å–ª–µ–¥—É—é—â–∏–π –∞–±–∑–∞—Ü
        # –∏ —Ç–µ–∫—É—â–∏–π –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å "—Ü–∏—Ñ—Ä–∞.(—Ü–∏—Ñ—Ä–∞).(—Ü–∏—Ñ—Ä–∞) ... "
        if i < n - 1 and heading_pattern.match(current_para):
            # –°–∫–ª–µ–∏–≤–∞–µ–º —Å —Å–ª–µ–¥—É—é—â–∏–º –∞–±–∑–∞—Ü–µ–º
            next_para = paragraphs[i+1].strip()
            merged = current_para + " " + next_para
            merged_paragraphs.append(merged)
            i += 2  # –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª–µ–¥—É—é—â–∏–π, —Ç.–∫. —É–∂–µ —Å–∫–ª–µ–∏–ª–∏
        else:
            # –ò–Ω–∞—á–µ –æ—Å—Ç–∞–≤–ª—è–µ–º –∫–∞–∫ –µ—Å—Ç—å
            merged_paragraphs.append(current_para)
            i += 1

    return merged_paragraphs



def chunk_paragraphs(paragraphs, max_tokens=512, overlap=0):
    """
    –†–∞–∑–±–∏–≤–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –∞–±–∑–∞—Ü–µ–≤ –Ω–∞ —á–∞–Ω–∫–∏ (–æ–∫–Ω–∞) —Ç–∞–∫,
    —á—Ç–æ–±—ã —Å—É–º–º–∞—Ä–Ω–æ–µ —á–∏—Å–ª–æ —Ç–æ–∫–µ–Ω–æ–≤ –≤ —á–∞–Ω–∫–µ –Ω–µ –ø—Ä–µ–≤—ã—à–∞–ª–æ max_tokens.

    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
      paragraphs: —Å–ø–∏—Å–æ–∫ —Å—Ç—Ä–æ–∫ (–∞–±–∑–∞—Ü–µ–≤), –Ω–∞–ø—Ä–∏–º–µ—Ä merged.
      max_tokens: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ —á–∏—Å–ª–æ '—Ç–æ–∫–µ–Ω–æ–≤' (–∑–¥–µ—Å—å —Å—á–∏—Ç–∞–µ–º –ø–æ —Å–ª–æ–≤–∞–º).
      overlap: –∫–æ–ª-–≤–æ —Å–ª–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–≤—Ç–æ—Ä—è–µ–º –≤ –Ω–∞—á–∞–ª–µ —Å–ª–µ–¥—É—é—â–µ–≥–æ —á–∞–Ω–∫–∞
               (—Å–∫–æ–ª—å–∑—è—â–µ–µ –æ–∫–Ω–æ). –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é 0 ‚Äî –±–µ–∑ –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è.

    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
      chunks: —Å–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤, –≥–¥–µ –∫–∞–∂–¥—ã–π —á–∞–Ω–∫ ‚Äî —ç—Ç–æ —Å—Ç—Ä–æ–∫–∞ (—Å–∫–ª–µ–µ–Ω–Ω—ã–µ –∞–±–∑–∞—Ü—ã).
    """
    chunks = []
    current_chunk = []
    current_token_count = 0

    for para in paragraphs:
        # –ü–æ–¥—Å—á–∏—Ç–∞–µ–º –∫–æ–ª-–≤–æ —Å–ª–æ–≤ (—Ç–æ–∫–µ–Ω–æ–≤) –≤ –∞–±–∑–∞—Ü–µ
        para_tokens = word_tokenize(para)
        para_len = len(para_tokens)

        # –ï—Å–ª–∏ –∞–±–∑–∞—Ü –æ–¥–∏–Ω —Å–∞–º –ø–æ —Å–µ–±–µ –¥–ª–∏–Ω–Ω–µ–µ max_tokens,
        # –ø—Ä–∏–¥—ë—Ç—Å—è —Ä–∞–∑–±–∏—Ç—å –µ–≥–æ –≤–Ω—É—Ç—Ä–∏
        if para_len > max_tokens:
            # –°—Ä–∞–∑—É –∑–∞–ª–∏–≤–∞–µ–º –≤—Å—ë –≤–Ω—É—Ç—Ä—å, —Ä–∞–∑–±–∏–≤–∞—è –Ω–∞ –ø–æ–¥—á–∞–Ω–∫–∏
            start_idx = 0
            while start_idx < para_len:
                end_idx = start_idx + (max_tokens - overlap)
                sub_tokens = para_tokens[start_idx:end_idx]

                if overlap > 0 and end_idx < para_len:
                    # –î–æ–±–∞–≤–∏–º –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ –≤ –∫–æ–Ω–µ—Ü —Ç–µ–∫—É—â–µ–≥–æ —á–∞–Ω–∫–∞
                    # (sub_tokens + —á–∞—Å—Ç—å —Å–ª–µ–¥—É—é—â–∏—Ö)
                    # –ù–æ —á–∞—â–µ –¥–µ–ª–∞—é—Ç overlap –ø—Ä–∏ –ø–µ—Ä–µ—Ö–æ–¥–µ –∫ —Å–ª–µ–¥—É—é—â–µ–º—É —á–∞–Ω–∫—É.
                    pass

                # –§–æ—Ä–º–∏—Ä—É–µ–º —Å—Ç—Ä–æ–∫—É –∏–∑ —Å—É–±-—Ç–æ–∫–µ–Ω–æ–≤
                sub_chunk_text = " ".join(sub_tokens)

                # –ï—Å–ª–∏ –µ—Å—Ç—å —É–∂–µ –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã–µ –∞–±–∑–∞—Ü—ã –≤ current_chunk,
                # —Å–Ω–∞—á–∞–ª–∞ –¥–æ–±–∞–≤–∏–º –∏—Ö, —á—Ç–æ–±—ã —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –ª–æ–≥–∏–∫—É —Å–∫–ª–µ–π–∫–∏
                if current_chunk:
                    # –°–∫–ª–µ–∏–º –≤—Å—ë –≤ –æ–¥–∏–Ω —Ç–µ–∫—Å—Ç
                    prev_text = " ".join(current_chunk)
                    combined_text = prev_text + " " + sub_chunk_text
                else:
                    combined_text = sub_chunk_text

                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Ç–æ–≥–æ–≤—ã–π —á–∞–Ω–∫
                chunks.append(combined_text.strip())

                # –û—á–∏—â–∞–µ–º current_chunk, —Ç–∞–∫ –∫–∞–∫ –º—ã ¬´–∑–∞–∫—Ä—ã–ª–∏¬ª –µ–≥–æ
                current_chunk = []
                current_token_count = 0

                start_idx = end_idx - overlap  # —Å–¥–≤–∏–≥–∞–µ–º—Å—è —Å —É—á—ë—Ç–æ–º overlap

            continue  # –ø–µ—Ä–µ—Ö–æ–¥–∏–º –∫ —Å–ª–µ–¥—É—é—â–µ–º—É –∞–±–∑–∞—Ü—É

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –≤–ª–µ–∑–∞–µ—Ç –ª–∏ —ç—Ç–æ—Ç –∞–±–∑–∞—Ü –≤ —Ç–µ–∫—É—â–∏–π —á–∞–Ω–∫
        if current_token_count + para_len <= max_tokens:
            # –ü—Ä–æ—Å—Ç–æ –¥–æ–±–∞–≤–ª—è–µ–º –∞–±–∑–∞—Ü –≤ —Ç–µ–∫—É—â–∏–π —á–∞–Ω–∫
            current_chunk.append(para)
            current_token_count += para_len
        else:
            # –¢–µ–∫—É—â–∏–π –∞–±–∑–∞—Ü –Ω–µ –≤–ª–µ–∑–∞–µ—Ç ‚Äî –∑–∞–∫—Ä—ã–≤–∞–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏–π —á–∞–Ω–∫
            if current_chunk:
                chunks.append(" ".join(current_chunk).strip())

            # –ù–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—ã–π —á–∞–Ω–∫ —Å —ç—Ç–æ–≥–æ –∞–±–∑–∞—Ü–∞
            current_chunk = [para]
            current_token_count = para_len

    # –ï—Å–ª–∏ —á—Ç–æ-—Ç–æ –æ—Å—Ç–∞–ª–æ—Å—å –Ω–µ–∑–∞–∫—Ä—ã—Ç—ã–º –≤ current_chunk ‚Äî –¥–æ–±–∞–≤–∏–º
    if current_chunk:
        chunks.append(" ".join(current_chunk).strip())

    # –ï—Å–ª–∏ –Ω—É–∂–µ–Ω overlap –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏, –º–æ–∂–Ω–æ —É—Å–ª–æ–∂–Ω–∏—Ç—å –ª–æ–≥–∏–∫—É,
    # –ø–æ–≤—Ç–æ—Ä—è—è –ø–æ—Å–ª–µ–¥–Ω–∏–µ overlap —Å–ª–æ–≤ –æ–¥–Ω–æ–≥–æ —á–∞–Ω–∫–∞ –≤ –Ω–∞—á–∞–ª–µ —Å–ª–µ–¥—É—é—â–µ–≥–æ.
    # –ó–¥–µ—Å—å –ø–æ–∫–∞–∑–∞–Ω –±–∞–∑–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥.

    return chunks



def embed_chunks(chunks, model):
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –≤–µ–∫—Ç–æ—Ä–æ–≤-—ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —á–∞–Ω–∫–∞.
    """
    return model.encode(chunks, convert_to_numpy=True)

def search_chunks(query, chunk_embeddings, chunks, model, top_k=3):
    """
    –ò—â–µ—Ç —Ç–æ–ø-K —Å–∞–º—ã—Ö —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤ –∫ –∑–∞–ø—Ä–æ—Å—É.

    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
      query: —Å—Ç—Ä–æ–∫–∞ (–∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è)
      chunk_embeddings: np.array, —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —á–∞–Ω–∫–æ–≤
      chunks: —Å–ø–∏—Å–æ–∫ —Å–∞–º–∏—Ö —á–∞–Ω–∫–æ–≤ (—Ç–µ–∫—Å—Ç)
      model: –º–æ–¥–µ–ª—å SentenceTransformer
      top_k: —Å–∫–æ–ª—å–∫–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å

    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (—Ç–µ–∫—Å—Ç_—á–∞–Ω–∫–∞, —Å—Ö–æ–¥—Å—Ç–≤–æ).
    """
    # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –∑–∞–ø—Ä–æ—Å–∞
    query_emb = model.encode([query], convert_to_numpy=True)
    # –°—á–∏—Ç–∞–µ–º –∫–æ—Å–∏–Ω—É—Å–Ω—É—é –ø–æ—Ö–æ–∂–µ—Å—Ç—å —Å–æ –≤—Å–µ–º–∏ —á–∞–Ω–∫–∞–º–∏
    similarities = cosine_similarity(query_emb, chunk_embeddings)[0]
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –∏–Ω–¥–µ–∫—Å—ã –ø–æ —É–±—ã–≤–∞–Ω–∏—é —Å—Ö–æ–¥—Å—Ç–≤–∞
    top_indices = similarities.argsort()[-top_k:][::-1]
    # –°–æ–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    results = [(chunks[idx], similarities[idx]) for idx in top_indices]
    return results

def cosine_similarity(vec1, vec2):
    # –ï—Å–ª–∏ vec1 –∏–º–µ–µ—Ç —Ñ–æ—Ä–º—É (1, dim), –∞ vec2 ‚Äî (n, dim), —Ç–æ —Ç—Ä–∞–Ω—Å–ø–æ–Ω–∏—Ä—É–µ–º vec2
    if vec1.ndim == 2:
        dot_product = np.dot(vec1, vec2.T)
        norm1 = np.linalg.norm(vec1, axis=1, keepdims=True)
        norm2 = np.linalg.norm(vec2, axis=1)
        return dot_product / (norm1 * norm2)
    else:
        dot_product = np.dot(vec1, vec2)
        norm1 = np.linalg.norm(vec1)
        norm2 = np.linalg.norm(vec2, axis=1)
        return dot_product / (norm1 * norm2)


def main(query):
    pdf_path = "gigagan_cvpr2023_original1.pdf"
    text = extract_text_pdfminer(pdf_path)
    paragraphs = split_text_custom(text)

    
    merged = merge_heading_paragraphs(paragraphs)
    chunks = chunk_paragraphs(merged, max_tokens=256, overlap=10)

    print(f"\n–í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤: {len(chunks)}")

    
    model = SentenceTransformer("multi-qa-mpnet-base-dot-v1")
    # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è —á–∞–Ω–∫–æ–≤
    chunk_embeddings = embed_chunks(chunks, model)

    # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫ –ø–æ –∑–∞–ø—Ä–æ—Å—É
    top_results = search_chunks(query, chunk_embeddings, chunks, model, top_k=3)
    print("\nüîé –ó–∞–ø—Ä–æ—Å:", query)
    print("–¢–æ–ø-3 —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–∞:\n")
    for i, (text_chunk, score) in enumerate(top_results, start=1):
        print(f"{i}. –°—Ö–æ–¥—Å—Ç–≤–æ = {score:.4f}")
        print(f"–¢–µ–∫—Å—Ç —á–∞–Ω–∫–∞:\n{text_chunk}\n{'-'*50}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="–ü–æ–∏—Å–∫ –ø–æ PDF –¥–æ–∫—É–º–µ–Ω—Ç—É")
    parser.add_argument('--query', type=str, help="–ó–∞–ø—Ä–æ—Å –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ PDF", required=True)
    args = parser.parse_args()
    main(args.query)

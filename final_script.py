import os
import re
import argparse
import nltk
import numpy as np
from PIL import Image, ImageTk

import faiss
import torch

from nltk.tokenize import word_tokenize
from pdfminer.high_level import extract_text
from sentence_transformers import SentenceTransformer, util
import tkinter as tk

# ==============
# –ó–∞–≥—Ä—É–∑–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤ NLTK
# ==============
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('averaged_perceptron_tagger')
nltk.download('wordnet')
nltk.download('omw-1.4')

# =====================
# –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã, –ø—É—Ç–∏
# =====================
PDF_PATH = "gigagan_cvpr2023_original1.pdf"
IMAGES_DIR = "/home/alex/Projects/Test Ranking/Test-Ranking/images"
MAX_TOKENS = 256   # –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –Ω–∞ —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞
OVERLAP = 10       # –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ —á–∞–Ω–∫–æ–≤ –ø–æ —Å–ª–æ–≤–∞–º
TOP_K_TEXT = 3     # —Å–∫–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —á–∞–Ω–∫–æ–≤ –∏—â–µ–º
TOP_K_IMAGES = 3   # —Å–∫–æ–ª—å–∫–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —á–∞–Ω–∫–∞

# =====================
# –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è PDF
# =====================
def extract_text_pdfminer(pdf_path):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ PDF —Å –ø–æ–º–æ—â—å—é pdfminer.six"""
    return extract_text(pdf_path)

def split_text_custom(text):
    """
    –î–µ–ª–∏–º —Ç–µ–∫—Å—Ç –ø–æ \n\n, –≤–Ω—É—Ç—Ä–∏ –∫–∞–∂–¥–æ–≥–æ –±–ª–æ–∫–∞ —É–±–∏—Ä–∞–µ–º \n,
    –∑–∞—Ç–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –¥–µ–ª–∏–º –ø–æ \x0c –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–ø–∏—Å–æ–∫ —á–∏—Å—Ç—ã—Ö –∞–±–∑–∞—Ü–µ–≤.
    """
    raw_blocks = text.split("\n\n")
    final_paragraphs = []

    for block in raw_blocks:
        block = block.replace("\n", " ")
        sub_blocks = block.split("\x0c")
        for sb in sub_blocks:
            sb = sb.strip()
            if sb:
                final_paragraphs.append(sb)
    return final_paragraphs

def merge_heading_paragraphs(paragraphs):
    """
    –ò—â–µ—Ç –∞–±–∑–∞—Ü—ã, –∫–æ—Ç–æ—Ä—ã–µ –Ω–∞—á–∏–Ω–∞—é—Ç—Å—è —Å —Ü–∏—Ñ—Ä –∏ –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –∏—Ö —Å –ø–æ—Å–ª–µ–¥—É—é—â–∏–º –∞–±–∑–∞—Ü–µ–º.
    """
    heading_pattern = re.compile(r'^[0-9]+(?:\.[0-9]+)*(?:\.)?\s+')
    merged_paragraphs = []
    i = 0
    n = len(paragraphs)

    while i < n:
        current_para = paragraphs[i].strip()
        if i < n - 1 and heading_pattern.match(current_para):
            next_para = paragraphs[i+1].strip()
            merged = current_para + " " + next_para
            merged_paragraphs.append(merged)
            i += 2
        else:
            merged_paragraphs.append(current_para)
            i += 1

    return merged_paragraphs

def chunk_paragraphs(paragraphs, max_tokens=512, overlap=0):
    """
    –†–∞–∑–±–∏–≤–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –∞–±–∑–∞—Ü–µ–≤ –Ω–∞ —á–∞–Ω–∫–∏ —Ç–∞–∫, —á—Ç–æ–±—ã —Å—É–º–º–∞—Ä–Ω–æ–µ —á–∏—Å–ª–æ —Ç–æ–∫–µ–Ω–æ–≤ –Ω–µ –ø—Ä–µ–≤—ã—à–∞–ª–æ max_tokens.
    overlap - –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–≤—Ç–æ—Ä—è—é—Ç—Å—è –≤ –Ω–∞—á–∞–ª–µ —Å–ª–µ–¥—É—é—â–µ–≥–æ —á–∞–Ω–∫–∞.
    """
    chunks = []
    current_chunk = []
    current_token_count = 0

    for para in paragraphs:
        para_tokens = word_tokenize(para)
        para_len = len(para_tokens)

        if para_len > max_tokens:
            start_idx = 0
            while start_idx < para_len:
                end_idx = start_idx + (max_tokens - overlap)
                sub_tokens = para_tokens[start_idx:end_idx]
                sub_chunk_text = " ".join(sub_tokens)
                if current_chunk:
                    prev_text = " ".join(current_chunk)
                    combined_text = prev_text + " " + sub_chunk_text
                else:
                    combined_text = sub_chunk_text
                chunks.append(combined_text.strip())
                current_chunk = []
                current_token_count = 0
                start_idx = end_idx - overlap
            continue

        if current_token_count + para_len <= max_tokens:
            current_chunk.append(para)
            current_token_count += para_len
        else:
            if current_chunk:
                chunks.append(" ".join(current_chunk).strip())
            current_chunk = [para]
            current_token_count = para_len

    if current_chunk:
        chunks.append(" ".join(current_chunk).strip())

    return chunks

def embed_chunks(chunks, model):
    """–í—ã—á–∏—Å–ª—è–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è —Å–ø–∏—Å–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —á–∞–Ω–∫–æ–≤."""
    return model.encode(chunks, convert_to_numpy=True)

def cosine_similarity(vec1, vec2):
    """
    –í—ã—á–∏—Å–ª—è–µ—Ç –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –º–µ–∂–¥—É vec1 (N x dim) –∏ vec2 (M x dim).
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –º–∞—Ç—Ä–∏—Ü—É —Å—Ö–æ–¥—Å—Ç–≤–∞ —Ä–∞–∑–º–µ—Ä–∞ N x M.
    """
    if vec1.ndim == 1:
        vec1 = vec1.reshape(1, -1)
    if vec2.ndim == 1:
        vec2 = vec2.reshape(1, -1)
    dot_product = np.dot(vec1, vec2.T)
    norm1 = np.linalg.norm(vec1, axis=1, keepdims=True)
    norm2 = np.linalg.norm(vec2, axis=1, keepdims=True)
    return dot_product / (norm1 * norm2.T)

def search_chunks(query, chunk_embeddings, chunks, model, top_k=3):
    """
    –ò—â–µ—Ç —Ç–æ–ø-K –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —á–∞–Ω–∫–æ–≤ –¥–ª—è –∑–∞–¥–∞–Ω–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (—Ç–µ–∫—Å—Ç_—á–∞–Ω–∫–∞, —Å—Ö–æ–¥—Å—Ç–≤–æ).
    """
    query_emb = model.encode([query], convert_to_numpy=True)
    sims = cosine_similarity(query_emb, chunk_embeddings)[0]
    top_indices = sims.argsort()[-top_k:][::-1]
    return [(chunks[idx], sims[idx]) for idx in top_indices]

# =====================
# –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ FAISS
# =====================
def get_all_image_paths(root_dir):
    """
    –†–µ–∫—É—Ä—Å–∏–≤–Ω–æ —Å–æ–±–∏—Ä–∞–µ—Ç –ø—É—Ç–∏ –∫–æ –≤—Å–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º –∏–∑ –∑–∞–¥–∞–Ω–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏.
    """
    valid_exts = ('.jpg', '.jpeg', '.png', '.bmp', '.gif', '.webp')
    image_paths = []
    for root, dirs, files in os.walk(root_dir):
        for file in files:
            if file.lower().endswith(valid_exts):
                full_path = os.path.join(root, file)
                image_paths.append(full_path)
    return image_paths

def build_image_faiss_index(image_paths, clip_model):
    """
    –í—ã—á–∏—Å–ª—è–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, —Å—Ç—Ä–æ–∏—Ç FAISS-–∏–Ω–¥–µ–∫—Å –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç (index, id_to_path).
    """
    embeddings = []
    valid_paths = []
    for path in image_paths:
        try:
            img = Image.open(path).convert('RGB')
            emb = clip_model.encode(img, convert_to_tensor=True)
            embeddings.append(emb)
            valid_paths.append(path)
        except Exception as e:
            print(f"Error with image {path}: {e}")

    if not embeddings:
        return None, {}

    embeddings = torch.stack(embeddings, dim=0)
    embeddings_np = embeddings.cpu().numpy().astype('float32')

    dim = embeddings_np.shape[1]
    index = faiss.IndexFlatIP(dim)
    # –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ –º–æ–∂–Ω–æ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å: faiss.normalize_L2(embeddings_np)
    index.add(embeddings_np)
    id_to_path = {i: p for i, p in enumerate(valid_paths)}
    return index, id_to_path

def encode_text_clip(text, clip_model):
    """
    –ü–æ–ª—É—á–∞–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥ —Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ CLIP-–º–æ–¥–µ–ª—å.
    """
    emb = clip_model.encode([text], convert_to_tensor=True)
    return emb.cpu().numpy().astype('float32')

def search_images_for_chunk_faiss(chunk_text, clip_model, faiss_index, id_to_path, top_k=3):
    """
    –î–ª—è –∑–∞–¥–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —á–∞–Ω–∫–∞ –∏—â–µ—Ç —Ç–æ–ø-K –ø–æ—Ö–æ–∂–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —á–µ—Ä–µ–∑ FAISS.
    """
    text_emb_np = encode_text_clip(chunk_text, clip_model)
    D, I = faiss_index.search(text_emb_np, top_k)
    D = D[0]
    I = I[0]
    results = []
    for rank, idx in enumerate(I):
        score = D[rank]
        img_path = id_to_path[idx]
        results.append((img_path, score))
    return results

def show_image_popup(image_path, window_title="Image"):
    """
    –û—Ç–∫—Ä—ã–≤–∞–µ—Ç –≤—Å–ø–ª—ã–≤–∞—é—â–µ–µ –æ–∫–Ω–æ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º –∏ –∑–∞–≥–æ–ª–æ–≤–∫–æ–º window_title.
    """
    popup = tk.Toplevel()
    popup.title(window_title)
    img = Image.open(image_path).convert('RGB')
    tk_img = ImageTk.PhotoImage(img)
    label = tk.Label(popup, image=tk_img)
    label.image = tk_img  # —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å —Å–±–æ—Ä–∫–∏ –º—É—Å–æ—Ä–∞
    label.pack()

# =====================
# –û—Å–Ω–æ–≤–Ω–∞—è –ª–æ–≥–∏–∫–∞
# =====================
def main():
    parser = argparse.ArgumentParser(description="–ü–æ–∏—Å–∫ –ø–æ PDF –¥–æ–∫—É–º–µ–Ω—Ç—É + (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ) –ø–æ–∏—Å–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —á–µ—Ä–µ–∑ FAISS")
    parser.add_argument('--query', type=str, required=True, help="–ó–∞–ø—Ä–æ—Å –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ PDF")
    parser.add_argument('--with_images', action='store_true',
                        help="–ï—Å–ª–∏ —É–∫–∞–∑–∞—Ç—å, –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –≤—ã–ø–æ–ª–Ω–∏—Ç—å –ø–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —á–µ—Ä–µ–∑ FAISS –∏ –æ—Ç–∫—Ä—ã—Ç—å –∏—Ö.")
    args = parser.parse_args()

    # 1) –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ PDF
    text = extract_text_pdfminer(PDF_PATH)
    paragraphs = split_text_custom(text)
    merged = merge_heading_paragraphs(paragraphs)
    chunks = chunk_paragraphs(merged, max_tokens=MAX_TOKENS, overlap=OVERLAP)
    print(f"\n–í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤: {len(chunks)}")

    # 2) –¢–µ–∫—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫ –ø–æ PDF —á–µ—Ä–µ–∑ mpnet
    text_model = SentenceTransformer("multi-qa-mpnet-base-dot-v1")
    chunk_embeddings = embed_chunks(chunks, text_model)
    top_text_results = search_chunks(args.query, chunk_embeddings, chunks, text_model, top_k=TOP_K_TEXT)

    print(f"\nüîé –ó–∞–ø—Ä–æ—Å: {args.query}")
    print(f"–¢–æ–ø-{TOP_K_TEXT} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —á–∞–Ω–∫–æ–≤:\n")
    for i, (text_chunk, score) in enumerate(top_text_results, start=1):
        print(f"{i}. –°—Ö–æ–¥—Å—Ç–≤–æ = {score:.4f}")
        print(f"–¢–µ–∫—Å—Ç —á–∞–Ω–∫–∞:\n{text_chunk}\n{'-'*50}")

    # 3) –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω —Ñ–ª–∞–≥ --with_images, –≤—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
    if args.with_images:
        print("\n--with_images: –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –ø–æ–∏—Å–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —á–µ—Ä–µ–∑ FAISS --")
        # –ó–∞–≥—Ä—É–∂–∞–µ–º CLIP-–º–æ–¥–µ–ª—å
        clip_model = SentenceTransformer("jinaai/jina-clip-v1", trust_remote_code=True)
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –ø—É—Ç–∏ –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º (–±–µ–∑ random.sample)
        image_paths = get_all_image_paths(IMAGES_DIR)
        print(f"–ù–∞–π–¥–µ–Ω–æ {len(image_paths)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ {IMAGES_DIR}")
        faiss_index, id_to_path = build_image_faiss_index(image_paths, clip_model)
        if faiss_index is None:
            print("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ—Å—Ç—Ä–æ–∏—Ç—å FAISS-–∏–Ω–¥–µ–∫—Å –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π.")
            return

        # –°–æ–∑–¥–∞—ë–º –≥–ª–∞–≤–Ω–æ–µ –æ–∫–Ω–æ TK –¥–ª—è –≤—Å–ø–ª—ã–≤–∞—é—â–∏—Ö –æ–∫–æ–Ω
        root = tk.Tk()
        root.withdraw()

        # –î–ª—è –∫–∞–∂–¥–æ–≥–æ –Ω–∞–π–¥–µ–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —á–∞–Ω–∫–∞ –∏—â–µ–º —Ç–æ–ø-K –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        for text_idx, (text_chunk, text_score) in enumerate(top_text_results, start=1):
            print(f"\n–ß–∞–Ω–∫ ‚Ññ{text_idx}, —Å—Ö–æ–¥—Å—Ç–≤–æ —Å –∑–∞–ø—Ä–æ—Å–æ–º = {text_score:.4f}")
            print(f"–¢–µ–∫—Å—Ç:\n{text_chunk}\n")
            img_results = search_images_for_chunk_faiss(text_chunk, clip_model, faiss_index, id_to_path, top_k=TOP_K_IMAGES)
            print(f"–¢–æ–ø-{TOP_K_IMAGES} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è —ç—Ç–æ–≥–æ —á–∞–Ω–∫–∞ (—á–µ—Ä–µ–∑ FAISS):")
            for img_idx, (img_path, img_score) in enumerate(img_results, start=1):
                print(f"{{text_index: {text_idx}, img_index: {img_idx}}}: –°—Ö–æ–¥—Å—Ç–≤–æ = {img_score:.4f} -> {img_path}")
                # –û—Ç–∫—Ä—ã–≤–∞–µ–º –≤—Å–ø–ª—ã–≤–∞—é—â–µ–µ –æ–∫–Ω–æ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º
                popup_title = f"text_index: {text_idx}, img_index: {img_idx}"
                show_image_popup(img_path, popup_title)
            print("-"*50)

        root.mainloop()  # –∑–∞–ø—É—Å–∫–∞–µ–º —Ü–∏–∫–ª TK

if __name__ == "__main__":
    main()
